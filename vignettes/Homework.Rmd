---
title: "Homework of 'Statistical Computing' course"
author: "Yingsi Liang"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 6
    fig_height: 4
encoding: UTF-8
vignette: >
  %\VignetteIndexEntry{Homework of 'Statistical Computing' course}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Homework1 - 09-18

## Exercise 3.4

### Question

+ generate random samples from a Rayleigh$f(x) = \frac{x}{\sigma^2} e^{-\frac{x^2}{2\sigma^2}}, \quad x \geq 0, \sigma>0$.
+ use the histogram to check the model.

### Solution

+ Calcutate the PDF of Rayleigh distribution.
+ Use the inverse TransformMethod.

First, the PDF of Rayleigh distribution is:

$$ F(x; \sigma) = \int_0^x \frac{t}{\sigma^2} e^{-\frac{t^2}{2\sigma^2}} \, dt $$

Use another variable$u = \frac{t^2}{2\sigma^2}$, so we have $du = \frac{t}{\sigma^2} \, dt$

And then

$$ F(x; \sigma) = \int_0^{\frac{x^2}{2\sigma^2}} e^{-u} \, du
                = \left[ -e^{-u} \right]_0^{\frac{x^2}{2\sigma^2}}
                = 1 - e^{-\frac{x^2}{2\sigma^2}}
$$

So the PDF of Rayleigh distribution is $F(x; \sigma) = 1 - e^{-\frac{x^2}{2\sigma^2}} \quad \text{for } x \geq 0$

Second, according to the inverse TransformMethod, we need to do as follow:

Generate $U \sim \mathcal{U}(0,1)$

Return $X = F_{x}^{-1}(U)=\sigma\sqrt{-2log(1-U)}= \sigma\sqrt{-2log(U)}$ 

This situation is $\sigma=1$

```{r}
set.seed(88) 
n <- 1000
sigma <- 1
u <- runif(n)  # 生成 n 个 [0, 1) 之间的均匀随机数
rayleigh_n <- sigma * sqrt(-2 * log(u))  # Rayleigh 变换
hist(rayleigh_n, prob = TRUE, ylim = c(0, 0.8), main = expression(paste("Rayleigh Distribution ",sigma==1)), xlab = "x",  col = "lightblue")
# 绘制原本的图像
y <- seq(0,4,0.01)
yy <- (y/sigma^2)*exp(-y^2/(2*sigma^2))
lines(y,yy)
```

This situation is $\sigma=2$

```{r}
set.seed(88)
n <- 1000
sigma <- 2
u <- runif(n)  # 生成 n 个 [0, 1) 之间的均匀随机数
rayleigh_n <- sigma * sqrt(-2 * log(u))  # Rayleigh 变换
hist(rayleigh_n, prob = TRUE, ylim = c(0, 0.4), main = expression(paste("Rayleigh Distribution ",sigma==2)), xlab = "x",  col = "lightblue")
# 绘制原本的图像
y <- seq(0,8,0.01)
yy <- (y/sigma^2)*exp(-y^2/(2*sigma^2))
lines(y,yy)
```

This situation is $\sigma=3$

```{r}
set.seed(88)
n <- 1000
sigma <- 3
u <- runif(n)  # 生成 n 个 [0, 1) 之间的均匀随机数
rayleigh_n <- sigma * sqrt(-2 * log(u))  # Rayleigh 变换
hist(rayleigh_n, prob = TRUE, ylim = c(0, 0.3), main = expression(paste("Rayleigh Distribution ",sigma==3)), xlab = "x",  col = "lightblue")
# 绘制原本的图像
y <- seq(0,12,0.01)
yy <- (y/sigma^2)*exp(-y^2/(2*sigma^2))
lines(y,yy)
```

## Exercise 3.11

### Question

+ Generate a random sample of size 1000 from a normal location mixture.
+ The components of the mixture have $\mathcal{N}(0,1)$ and $\mathcal{N}(3,1)$ distributions with mixing probabilities $p_1$ and $p_2=1-p_1$. 

### Solution

Now $S=p_1F_{X_{1}}+p_2F_{X_{1}}$, where $X_1 \sim \mathcal{N}(0,1)$ and $X_2 \sim \mathcal{N}(0,1)$

```{R}
set.seed(123)
n <- 1000
u <- runif(n)
k <- as.integer(u>0.25)
x1 <- rnorm(1000,0,1)
x2 <- rnorm(1000,3,1)
x <- k*x1+k*x2
hist(x,prob=TRUE,col = "lightblue")
```

Use different $p_1$

```{r, echo=FALSE}
par(mfrow = c(2, 3))

set.seed(123)
n <- 1000
u <- runif(n)
k10 <- as.integer(u>0)
k11 <- as.integer(u>0.1)
k12 <- as.integer(u>0.2)
k13 <- as.integer(u>0.3)
k14 <- as.integer(u>0.4)
k15 <- as.integer(u>0.5)
x1 <- rnorm(1000,0,1)
x2 <- rnorm(1000,3,1)
x10 <- k10*x1+k10*x2
x11 <- k11*x1+k11*x2
x12 <- k12*x1+k12*x2
x13 <- k13*x1+k13*x2
x14 <- k14*x1+k14*x2
x15 <- k15*x1+k15*x2
hist(x10,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0)))
hist(x11,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.1)))
hist(x12,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.2)))
hist(x13,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.3)))
hist(x14,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.4)))
hist(x15,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.5)))

par(mfrow = c(1, 1))
```

We can find that when the values of $p_1$ is higher, it is easier to observe the mixture appears to be
bimodal. So we try to control the $p_1$ between 0 and 1.

```{r, echo=FALSE}
par(mfrow = c(2, 3))

set.seed(123)
n <- 1000
u <- runif(n)
k10 <- as.integer(u>0)
k11 <- as.integer(u>0.02)
k12 <- as.integer(u>0.04)
k13 <- as.integer(u>0.06)
k14 <- as.integer(u>0.08)
k15 <- as.integer(u>0.05)
x1 <- rnorm(1000,0,1)
x2 <- rnorm(1000,3,1)
x10 <- k10*x1+k10*x2
x11 <- k11*x1+k11*x2
x12 <- k12*x1+k12*x2
x13 <- k13*x1+k13*x2
x14 <- k14*x1+k14*x2
x15 <- k15*x1+k15*x2
hist(x10,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0)))
hist(x11,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.02)))
hist(x12,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.04)))
hist(x15,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.05)))
hist(x13,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.06)))
hist(x14,prob=TRUE,col = "lightblue", xlab='x', main = expression(paste("Distribution ",p1==0.08)))

par(mfrow = c(1, 1))
```

The result is if the values of $p_1$ is higer than $0.5$, it will produce bimodal mixtures.

## Exerise 3.20

### Question

+ Simulate a compound Poisson$(\lambda)$-Gamma process.
+ Estimate the mean and the variance of $X(10)$ for different choices of the parameters and compare with the theoretical values.

### Solution

First, calculate the mean and variance of compound Possion-Gamma process.

**Mean Calculation**

Expected Value of Poisson Process is: $E[N(t)] = \lambda t$

So

$$
E[X(t)] = E\left[\sum_{i=1}^{N(t)} Y_i\right] = E[N(t)] \cdot E[Y_i] = (\lambda t) E[Y_i]
$$

**Variance Calculation**

Law of total variance is: $Var(X(t)) = E[Var(X(t) | N(t))] + Var(E[X(t) | N(t)])$

and $E[Var(X(t) | N(t))] = E[N(t)] \cdot E(Y_1) = (\lambda t) \cdot E(Y_1)$

and $Var(E[X(t) | N(t)]) = Var(N(t) \cdot E(Y_1)) = (E(Y_1))^2 \cdot Var(N(t)) = (\lambda t) \cdot E(Y_1)$

So

$$
Var(X(t)) = (\lambda t) (E[Y_1])^2
$$

this situation is $\lambda=1$

```{r}
library(ggplot2)
set.seed(123)

# 设置参数
lambda <- 1          # Poisson 过程的平均事件数
alpha <- 2           # Gamma 分布的形状参数
beta <- 1            # Gamma 分布的尺度参数
T <- 10              # 模拟时间长度

simulate_compound_poisson_gamma <- function(lambda, alpha, beta, t) {
  # 生成 Poisson 随机变量 N
  N <- rpois(1, lambda * T)  # 在时间区间 T 内生成的事件数量
  # 生成 Gamma 随机变量并求和
  gamma_samples <- rgamma(N, shape = alpha, scale = beta)  # 生成 N 个 Gamma 样本
  total_amount <- sum(gamma_samples)  # 计算总和
  return(total_amount)
}
  
# 进行多次模拟
num_simulations <- 10000
results <- replicate(num_simulations, simulate_compound_poisson_gamma(lambda, alpha, beta, t))

# 计算均值和方差
mean_X <- mean(results)
var_X <- var(results)

# 输出结果
cat("X(10) 的均值:", mean_X, "\n")
cat("X(10) 的方差:", var_X, "\n")
cat("均值理论值为：",lambda*T*alpha*beta,"真实值与理论值的差距为：",abs(lambda*T*alpha*beta-mean_X),"\n")
cat("方差理论值为：",lambda*T*(alpha*beta^2+(alpha*beta)^2),"真实值与理论值的差距为：",abs(lambda*T*(alpha*beta^2+(alpha*beta)^2)-var_X), "\n")

```

this situation is $\lambda=5$

```{r}
library(ggplot2)
set.seed(123)

# 设置参数
lambda <- 5          # Poisson 过程的平均事件数
alpha <- 2           # Gamma 分布的形状参数
beta <- 1            # Gamma 分布的尺度参数
T <- 10              # 模拟时间长度

simulate_compound_poisson_gamma <- function(lambda, alpha, beta, t) {
  # 生成 Poisson 随机变量 N
  N <- rpois(1, lambda * T)  # 在时间区间 T 内生成的事件数量
  # 生成 Gamma 随机变量并求和
  gamma_samples <- rgamma(N, shape = alpha, scale = beta)  # 生成 N 个 Gamma 样本
  total_amount <- sum(gamma_samples)  # 计算总和
  return(total_amount)
}
  
# 进行多次模拟
num_simulations <- 10000
results <- replicate(num_simulations, simulate_compound_poisson_gamma(lambda, alpha, beta, t))

# 计算均值和方差
mean_X <- mean(results)
var_X <- var(results)

# 输出结果
cat("X(10) 的均值:", mean_X, "\n")
cat("X(10) 的方差:", var_X, "\n")
cat("均值理论值为：",lambda*T*alpha*beta,"真实值与理论值的差距为：",abs(lambda*T*alpha*beta-mean_X), "\n")
cat("方差理论值为：",lambda*T*(alpha*beta^2+(alpha*beta)^2),"真实值与理论值的差距为：",abs(lambda*T*(alpha*beta^2+(alpha*beta)^2)-var_X), "\n")

```

this situation is $\lambda=10$

```{r}
library(ggplot2)
set.seed(123)

# 设置参数
lambda <- 10          # Poisson 过程的平均事件数
alpha <- 2           # Gamma 分布的形状参数
beta <- 1            # Gamma 分布的尺度参数
T <- 10              # 模拟时间长度

simulate_compound_poisson_gamma <- function(lambda, alpha, beta, t) {
  # 生成 Poisson 随机变量 N
  N <- rpois(1, lambda * T)  # 在时间区间 T 内生成的事件数量
  # 生成 Gamma 随机变量并求和
  gamma_samples <- rgamma(N, shape = alpha, scale = beta)  # 生成 N 个 Gamma 样本
  total_amount <- sum(gamma_samples)  # 计算总和
  return(total_amount)
}
  
# 进行多次模拟
num_simulations <- 10000
results <- replicate(num_simulations, simulate_compound_poisson_gamma(lambda, alpha, beta, t))

# 计算均值和方差
mean_X <- mean(results)
var_X <- var(results)

# 输出结果
cat("X(10) 的均值:", mean_X, "\n")
cat("X(10) 的方差:", var_X, "\n")
cat("均值理论值为：",lambda*T*alpha*beta,"真实值与理论值的差距为：",abs(lambda*T*alpha*beta-mean_X), "\n")
cat("方差理论值为：",lambda*T*(alpha*beta^2+(alpha*beta)^2),"真实值与理论值的差距为：",abs(lambda*T*(alpha*beta^2+(alpha*beta)^2)-var_X), "\n")

```

# Homework2 - 09-26

## Exercise 5.4

### Question

+ 计算$Beta(3,3)$密度函数的蒙特卡洛估计值。
+ 估计分布函数，其中$x=0.1,0.2,...,0.9$，并与R中的内置函数进行比较。

### Solution

贝塔分布的密度函数为：

$$
f(x; \alpha, \beta) = \frac{x^{\alpha-1} (1 - x)^{\beta-1}}{B(\alpha, \beta)}, \quad 0 < x < 1, \quad
B(\alpha, \beta) = \int_0^1 t^{\alpha-1} (1 - t)^{\beta-1} \, dt
$$
如果取$\alpha=3,\beta=3$，则密度函数为：

$$
f(x; 3, 3) = \frac{x^{2} (1 - x)^{2}}{B(3, 3)}=30 \cdot x^{2} (1 - x)^{2}, \quad 0 < x < 1
$$
令$y = \frac{t}{x}$，得到$dt = xdy$,则分布函数为：
$$
\theta = F(x;3,3)= \int_0^x 30 \cdot t^{2} (1 - t)^{2} dt = \int_0^1 30 \cdot x^{3}y^{2} (1 - xy)^{2} dy
$$
因此$\theta = E_Y[30 \cdot x^{3}Y^{2} (1 - xY)^{2}]$，其中随机变量$Y \sim U(0,1)$，可以通过生成独立同分布的服从均匀分布的随机数，然后计算分布函数。

生成简单蒙特卡洛估计CDF的函数为：
```{r}
# 设置参数
set.seed(12345)  # 固定随机数种子
m <- 10000  # 蒙特卡罗模拟次数
y <- runif(m)

# Beta分布的概率密度函数
beta_cdf <- function(x){
  return (mean(30 * x^3 * y^2 * (1-x * y)^2))
}
```

利用上述函数估计分布函数，并与内置函数$pbata$比较为：
```{r}
x <- seq(0.1,1,length=10)
cdf_beta <- numeric(length(t))
for (i in 1:length(x)){
  # 估计CDF
  cdf_beta[i] <- beta_cdf(x[i])
}
# 标准
Phi <- pbeta(x,3,3)
round(rbind(x, cdf_beta, Phi), 3)
```
## Exercise 5.9

### Question

+ 利用对偶变量的方法生成瑞利分布的样本。
+ 对比利用对偶变量下方差的减小值。

### Solution

瑞利分布的密度函数为：
$$ F(x; \sigma) = \frac{t}{\sigma^2} e^{-\frac{t^2}{2\sigma^2}} \, dt \quad x \geq 0,\sigma >0$$
计算瑞利分布的分布函数为：
$$ F(x; \sigma) =\int_0^x \frac{t}{\sigma^2} e^{-\frac{t^2}{2\sigma^2}} \, dt
                = \int_0^{\frac{x^2}{2\sigma^2}} e^{-u} \, du
                = \left[ -e^{-u} \right]_0^{\frac{x^2}{2\sigma^2}}
                = 1 - e^{-\frac{x^2}{2\sigma^2}}
$$
利用简单蒙特卡洛积分法和对偶变量法模拟瑞利分布：
```{r}
# 设置参数
set.seed(123)  # 固定随机数种子
m <- 10000  # 蒙特卡罗模拟次数

# 模拟函数 Rayleigh分布的概率密度函数
Rayleigh_cdf <- function(x, sigma, antithetic = TRUE) {
  u <- runif(m/2)
  if (!antithetic) v <- runif(m/2) else v<- (1 - u)
  u <- c(u, v)
  cdf_ray <- numeric(length(x))
  for (i in 1:length(x)) {
    cdf_ray[i] <- (mean((x[i]^2 * u / sigma^2) *exp(-(x[i]*u)^2/(2* sigma^2))))
  }
  cdf_ray
}
```

与标准的瑞利分布进行比较:

```{r}
# 标准的瑞利分布
sigma <- 2
x <- seq(0.1,2,length=20)

Phi_ray <- (1-exp(-x^2 / (2*sigma^2)))
cdf_ray1<- Rayleigh_cdf(x,sigma,antithetic=FALSE)
cdf_ray2<- Rayleigh_cdf(x,sigma)

round(rbind(x, cdf_ray1, cdf_ray2, Phi_ray), 4)
```

计算方差：
```{r}
m <- 1000
MC1 <- MC2 <- numeric(m)
x <- 1
for (i in 1:m) {
MC1[i] <- Rayleigh_cdf(x, sigma, anti = FALSE)
MC2[i] <- Rayleigh_cdf(x, sigma)
}
print(sd(MC1))
print(sd(MC2))
print((var(MC1) - var(MC2))/var(MC1))
```

## Exercise 5.13

### Question

+ 选择两个重要性函数，需要接近函数$g(x)=\frac{x^2} {\sqrt{2 \pi}} e^{-x^2 /2} ,\quad x>1$。
+ 比较两个重要性函数的模拟结果。

### Solution

选择的重要性函数为：
$$ 
f_1(x) = \left\{ \begin{array}{ll}
e^{-x} & \text{当 } x>1 \\
0 & \text{其他} 
\end{array}
\right.
$$
$$
f_2(x) = \left\{ \begin{array}{ll}
\frac{1}{\sqrt{2\pi}} e^{-\frac{(x-1)^2}{2}} & \text{当 } x>1 \\
0 & \text{其他} 
\end{array}
\right.
$$

进行模拟：
```{r}
m <- 10000 #模拟次数
set.seed(123)

theta.hat <- se <- numeric(2)

# 原函数
g <- function(x) {
  exp(-x^2/2 + log((x^2)/sqrt(2*pi))) * (x > 1)
}

# 重要性函数1
x1 <- rexp(m, 1) #生成指数分布的随机数
i <- c(which(x1 < 1))
x1[i] <- 0
fg1 <- g(x1) / exp(-x1)
theta.hat[1] <- mean(fg1)
se[1] <- sd(fg1)

# 重要性函数2
x2 <- rnorm(m,1,1) #生成正态分布的随机数
i <- c(which(x2 < 1))
x2[i] <- 0
fg2 <- g(x2) / dnorm(x2,1,1)
theta.hat[2] <- mean(fg2)
se[2] <- sd(fg2)

rbind(theta.hat, se)
```
对比可知，重要性函数$f_2(x)$更好，因为他有更小的方差。

后面进一步绘制图像发现，重要性函数$f_2(x)$更加贴近原本函数。

```{R}
k<-seq(0,5,0.01)
kk<-g(k)
plot(k,kk)
plot(k,exp(-k)*(k>1))
kkk<-1/sqrt(2*pi)*exp(-(k-1)^2/2)*(k>1)
plot(k,kkk)
```

## Exercise

### Question

+ 实现快速排序法
+ 在不同的数量级下测试快速排序法的时间
+ 数量级与运行时间的回归

### Solution

首先，实现快速排序法，并输出运行时间。
```{R}
n = c(1e4,2*1e4,4*1e4,6*1e4,8*1e4)
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}
time = numeric(length(n))
for (i in 1:length(n)){
  test<-sample(1:(n[i]))
  sort_result = quick_sort(test)
  time[i]=system.time(quick_sort(test))[1]
}
# system.time(quick_sort(sample(1:(2*1e4))))[1]
```
接下来，重复模拟100次并计算平均时间$a_n$。
```{R,eval = FALSE}
# m是模拟次数
# time_vc存储每一次模拟的时间，是一个1*m的向量
# time_mean存储平均时间，是一个1*5的向量
m=100
time_vc = numeric(m)
time_mean = numeric(length(n))
for (i in 1:length(n)){
  for (j in 1:m){
    test<-sample(1:(n[i]))
    time_vc[j] = system.time(quick_sort(test))[1]
  }
  time_mean[i] = mean(time_vc)
}
print(time_mean)
```

最后，对$a_n$和$t_n=nlog(n)$进行回归。

```{R,eval = FALSE}
nn = seq(1,4*1e4,5*1e3)
time_mean_ = numeric(length(nn))
for (i in 1:length(nn)){
  for (j in 1:m){
    test<-sample(1:(nn[i]))
    time_vc[j] = system.time(quick_sort(test))[1]
  }
  time_mean_[i] = mean(time_vc)
}
print(time_mean_)
```
绘制示意图
```{R}
# 运行时间太长了这里记录运行结果
nn = seq(1,4*1e4,5*1e3)
time_mean_ = c(0.0000,0.0085,0.0122,0.0260,0.0320,0.0404,0.0431,0.0467)
model <- lm(time_mean_ ~ nn) 
plot(nn,time_mean_,main = "散点图及回归线", xlab = "nlog(n)", ylab = "平均运行时间",pch = 19, col = "blue")
abline(model, col = "red", lwd = 2)  # 使用回归模型绘制回归线
```

# Homework3 - 09-30

## Exercise 6.6

### Question

+ 利用蒙特卡洛估计正态性下的样本偏度的0.025、0.05、0.95、0.975的分位数。
+ 计算估计值的标准误。
+ 估计值与大样本近似下的分位数进行比较。

### Solution

计算大样本近似下的分位数，分别取$n=50,100,500$，将结果存储在矩阵`cv`里。

计算样本偏度系数$\sqrt{b_1}$的分位数，并计算估计值的方差。其中，方差的计算公式如下：
$$
Var(\hat{x_q})=\frac{q(1-q)}{nf(x_q)^2}
$$
其中，$f$是采样分布的密度，$x_q$是分位数。

```{R}
get_data <- function(n,mu,sigma){rnorm(n,mu,sigma)}
calculate_data <- function(sample.x){
  mu <- mean(sample.x)
  sigma <- sd(sample.x)
  SK <- 1/n/sigma^3*sum((sample.x-mu)^3)
  return(SK)
}
show_data <- function(SK.li, q.li, m, n){
  hat.SK.q <- quantile(SK.li,q.li)
  sd.Sk.q <- sqrt(q.li*(1-q.li)/m/dnorm(hat.SK.q)^2)
  SK.q_large <- sqrt(6/n)*qnorm(q.li)
  return (data.frame('large.sample'=SK.q_large,'estimate'=hat.SK.q,'estimate.sd'=sd.Sk.q))
}
m <- 1000
n <- 50000
mu <- 0
sigma <- 1
hat.SK <- numeric(m)
q.li <- c(0.025, 0.05, 0.95, 0.975)
for (j in 1:m){
  sample.j <- get_data(n,mu,sigma)
  hat.SK[j] <- calculate_data(sample.j)
}
show_data(hat.SK,q.li,m,n)
```


## Exercise 6.B

### Question

+ 计算皮尔逊相关系数、斯皮尔曼秩相关系数、肯德尔相关系数，并作用于二元正态分布上。
+ 找到非参数检验更有效的二元分布

### Solution

皮尔逊相关系数用于度量两个变量之间的线性关系。相关系数的取值范围为$[-1, 1]$，其中 1 表示完全正相关，-1 表示完全负相关，0 表示无线性关系。

斯皮尔曼秩相关系数用于度量两个变量之间的单调关系，而不仅限于线性关系。适用于非线性关系的情况。

肯德尔相关系数用于度量两个变量之间的一致性。它与斯皮尔曼秩相关系数类似，但计算方法不同，基于成对数据的顺序一致性进行计算。

首先，生成二元正态分布随机数，并实现三种相关性度量的方法。

```{R}
get_data <- function(n){
  x <- rnorm(n)
  y <- rnorm(n)
  return(list(x=x,y=y))
}
calculate_data <- function(sample.x,sample.y){
  x <- sample.x
  y <- sample.y
  p1 <- cor.test(x,y,method='pearson')$p.value
  p2 <- cor.test(x,y,method='kendall')$p.value
  p3 <- cor.test(x,y,method='spearman')$p.value
  return(list(p1=p1,p2=p2,p3=p3))
}
show_data <- function(p1.li,p2.li,p3.li, alpha){
  c(mean(p1.li<=alpha),mean(p2.li<=alpha),mean(p3.li<=alpha))
}
m <- 5e4
n <- 10
alpha <- 0.05
p1 <- p2 <- p3 <- numeric(m)
for (j in 1:m){
  sample.j <- get_data(n)
  x <- sample.j$x
  y <- sample.j$y
  p.j <- calculate_data(x,y)
  p1[j] <- p.j$p1
  p2[j] <- p.j$p2
  p3[j] <- p.j$p3
}
show_data(p1,p2,p3,alpha)
```


容易看出，此时，⽪尔逊相关系数检验的结果0.05008最接近于0.05。

对于非正态假设、包含离群值、样本量较小、数据非线性的情况下，非参数检验的效果更好，具体见下例：

```{R}
get_data <- function(n){
    y <- rexp(n)
    x <- 1 + 1.5*y^{0.5} + rt(n,2)
    return(list(x=x,y=y))
 }
 m <- 5e3
 n <- 100
 alpha <- 0.05
 p1 <- p2 <- p3 <- numeric(m)
 for (j in 1:m){
    sample.j <- get_data(n)
    x <- sample.j$x
    y <- sample.j$y
    p.j <- calculate_data(x,y)
    p1[j] <- p.j$p1
    p2[j] <- p.j$p2
    p3[j] <- p.j$p3
 }
 show_data(p1,p2,p3,alpha)
```

两种⾮参数检验的功效⼤概为0.98，均⼤于⽪尔逊相关系数检验的功效0.75。

## Exercise 3

### Question

+ 判断两种方法的功效是否存在显著性差异
  + 假设检验是什么样的
  + 应该使用什么检验
  
### Solution

设两个功效用$p_1,p_2$表示，则假设检验为：

$$
H_0:p_1=p_2,\quad H_1:p_1 \neq p_2
$$

功效实际上是第一类错误的比例，因此可以利用$Z$检验$Z-test$，即为：

$$
Z=\frac{(p_1-p_2)}{\sqrt{P(1-P)(\frac{1}{n_1}+\frac{1}{n_2})}}
$$

其中，$p_1,p_2$为第一、二种方法的功效，$n_1,n_2$为实验次数，$P=\frac{x_1+x_2}{n_1+n_2}$，$x_2,x_2$是成功的数量。下面进行计算：

```{R}
n <- 10000
x.bar <- 0.651
y.bar <- 0.676
p.hat <- 0.6635
z <- (x.bar-y.bar)/sqrt(2*p.hat*(1-p.hat)/n)
p.value <- 2*min(pnorm(z),1-pnorm(z))
p.value
```

计算得到$p= 0.000183<0.05$，拒绝原假设说明两种方法有明显的差距。

# Homework4 - 10-16


## Exercise

### Question

+ 利用Bonferroni和B-H方法调整p值。
+ 在10000次模拟和$\alpha=0.1$下计算FWER、FDP、TPR

### Answer

实现方法：

+ 按照题目要求生成服从均匀分布和beta分布的p值
+ 利用内置函数进行p值的调整
+ 计算误差值，并对结果可视化

```{R}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)  # 设置随机数种子，保证结果可重复
# 参数设置
N <- 1000       
# 总假设数
n_null <- 950   # 原假设数
n_alt <- 50     
m <- 10000      
alpha <- 0.1    
# 初始化结果矩阵
# 备择假设数
# 模拟次数
# 显著性水平
results <- matrix(0, nrow = 3, ncol = 2)
rownames(results) <- c("FWER", "FDR", "TPR")
colnames(results) <- c("Bonferroni校正", "B-H校正")
 
 # 加载必要的库
library(ggplot2)
 # 初始化存储变量
fwer_bonf <- numeric(m)
fdr_bonf <- numeric(m)
tpr_bonf <- numeric(m)
fwer_bh <- numeric(m)
fdr_bh <- numeric(m)
tpr_bh <- numeric(m)
 # 执行模拟
for (i in 1:m) {
  # 生成 p 值
  p_null <- runif(n_null)               # 原假设的 p 值
  p_alt <- rbeta(n_alt, 0.1, 1)         # 备择假设的 p 值
  p_values <- c(p_null, p_alt)          # 合并所有 p 值
  true_labels <- c(rep(0, n_null), rep(1, n_alt))  # 标签：0-原假设，1-备择假设
  
  # Bonferroni 校正
  p_bonf <- p.adjust(p_values, method = "bonferroni")
  rejected_bonf <- which(p_bonf < alpha)
  
  V_bonf <- sum(true_labels[rejected_bonf] == 0)  # 误拒的原假设数
  S_bonf <- sum(true_labels[rejected_bonf] == 1)  # 正确拒绝的备择假设数
  R_bonf <- length(rejected_bonf)                 # 总共拒绝的假设数
  
  fwer_bonf[i] <- as.numeric(V_bonf > 0)          # 至少一个假阳性
  fdr_bonf[i] <- ifelse(R_bonf > 0, V_bonf / R_bonf, 0)  # 错误发现率
  tpr_bonf[i] <- S_bonf / n_alt                   # 真阳性率
  
  # B-H 校正
  p_bh <- p.adjust(p_values, method = "BH")
  rejected_bh <- which(p_bh < alpha)
  
  V_bh <- sum(true_labels[rejected_bh] == 0)
  S_bh <- sum(true_labels[rejected_bh] == 1)
  R_bh <- length(rejected_bh)
  
  fwer_bh[i] <- as.numeric(V_bh > 0)
  fdr_bh[i] <- ifelse(R_bh > 0, V_bh / R_bh, 0)
  tpr_bh[i] <- S_bh / n_alt
  
  # 可视化 p 值分布（密度图）
  if (i == 1) { # 只绘制第一次模拟的 p 值分布
    # 创建一个数据框，用于 ggplot2 绘图
    p_data <- data.frame(
      p_value = p_values,
      hypothesis = factor(true_labels, labels = c("Null Hypothesis", "Alternative Hypothesis"))
    )
    
    # 绘制密度图
    p_plot <- ggplot(p_data, aes(x = p_value, fill = hypothesis, color = hypothesis)) +
      geom_density(alpha = 0.4) +  # 绘制密度曲线
      geom_vline(xintercept = alpha, linetype = "dashed", color = "red") +  # 显著性水平线
      labs(title = "P-value Density for Null and Alternative Hypotheses",
           x = "P-value",
           y = "Density") +
      theme_minimal()
    
    # 打印图形
    print(p_plot)
  }
  
  # 可视化 p 值分布（散点图）
  if (i == m) { # 只绘制第m次模拟的 p 值分布
    # 创建一个数据框，用于 ggplot2 绘图
    p_data <- data.frame(
      p_value = p_values,
      hypothesis = factor(true_labels, labels = c("Null Hypothesis", "Alternative Hypothesis"))
    )
  
    # 绘制散点图
    p_plot <- ggplot(p_data, aes(x = hypothesis, y = p_value, color = hypothesis)) +
      geom_jitter(width = 0.2, height = 0) +  # 使用散点抖动效果
      geom_hline(yintercept = alpha, linetype = "dashed", color = "red") +  # 显著性水平线
      labs(title = "P-value Distribution for Null and Alternative Hypotheses",
         x = "Hypothesis Type",
         y = "P-value") +
      theme_minimal()
  
    # 打印图形
    print(p_plot)
  }
}

```

下面输出结果：

```{R}
# 计算平均值
results["FWER", "Bonferroni校正"] <- mean(fwer_bonf)
results["FDR", "Bonferroni校正"] <- mean(fdr_bonf)
results["TPR", "Bonferroni校正"] <- mean(tpr_bonf)
results["FWER", "B-H校正"] <- mean(fwer_bh)
results["FDR", "B-H校正"] <- mean(fdr_bh)
results["TPR", "B-H校正"] <- mean(tpr_bh)
print(results)
```

从结果来看，可以发现Bonferroni校正比B-H校正更严格，可以更好地控制假阳性数量。Bonferroni校正由于更保守，通常会导致较低的真阳性检测率，而B-H校正则有更高的真阳性率。

## Exercise 7.4

### Question

+ 读取数据集aircondit
+ 假设数据服从指数模型下，求风险率的最大似然估计。
+ 使用bootstrap估计，并计算偏倚和标准误

指数分布的最大似然估计计算公式为
$$
\hat\lambda = \frac{n}{\sum_{i=1}^{n} x_i}
$$


```{R}
# 加载 boot 包和数据
library(boot)
data("aircondit")
x <- aircondit$hours
 # 样本大小
n <- length(x)
# 计算 λ 的 MLE
lambda_hat <- n / sum(x)
# 定义用于 Bootstrap 的统计量函数
boot_lambda <- function(data, indices) {
  # 从数据中抽样
  sample_data <- data[indices]
  # 计算 λ 的 MLE
  n_boot <- length(sample_data)
  sum_boot <- sum(sample_data)
  lambda_boot <- n_boot / sum_boot
  return(lambda_boot)
}
# 设置 Bootstrap 重复次数
R <- 10^5
# 进行 Bootstrap
set.seed(123)
boot_results <- boot(data = x, statistic = boot_lambda, R = R)

# 估计偏差
bias_lambda <- mean(boot_results$t) - lambda_hat
cat(sprintf("偏差估计为：%.6f\n", bias_lambda))
se_lambda <- sd(boot_results$t)
cat(sprintf("标准差估计为：%.6f\n", se_lambda))
```

## Exercise 7.5

### Question

+使用standard normal, basic, percentile和BCa方法计算上一题中的$\frac{1}{\lambda}$的95%的置信区间，并进行比较

### Answer

解决方法如下:

+ 首先要计算$\frac{1}{\lambda}$的均值
+ 利用Bootstrap采用估计$\frac{1}{\lambda}$的均值
+ 计算置信区间

```{R}
# 将 λ 转换为 θ = 1/λ
boot_theta <- boot_results
boot_theta$t <- 1 / boot_theta$t
theta_hat <- 1 / lambda_hat

# 定义一个用于 Bootstrap 方法的统计量函数，用于计算平均故障间隔时间（θ = 1/λ）
mean_time_boot <- function(data, indices) {
  # 从原始数据中抽取 Bootstrap 样本
  # 'data' 是原始数据集，'indices' 是抽样索引（包含有放回的抽样位置）
  sample_data <- data[indices]
  # 计算抽样样本的大小（观测值数量）
  n_boot <- length(sample_data)
  # 计算抽样样本中所有故障时间的总和
  sum_boot <- sum(sample_data)
  # 计算 Bootstrap 样本的最大似然估计（MLE）λ的估计值
  # λ 的 MLE 公式为 λ_hat = n / Σx_i，其中 n 是样本大小，Σx_i 是故障时间总和
  lambda_hat <- n_boot / sum_boot
  # 计算平均故障间隔时间 θ 的估计值，θ = 1/λ
  theta_boot <- 1 / lambda_hat
  # 返回 θ 的估计值作为 Bootstrap 样本的统计量
  return(theta_boot)
}
set.seed(133)  # 设置随机数种子，保证结果可重复
# 数据：空调设备的故障时间（小时）
failure_times <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
 # 使用bootstrap方法计算
bootstrap_mean_time_results <- boot(data = failure_times, statistic = mean_time_boot, R = 10000)

# 标准正态法置信区间
norm_ci <- boot.ci(bootstrap_mean_time_results, type = "norm")
 # 基本法置信区间
basic_ci <- boot.ci(bootstrap_mean_time_results, type = "basic")
 # 百分位法置信区间
perc_ci <- boot.ci(bootstrap_mean_time_results, type = "perc")
 # BCa 法置信区间
bca_ci <- boot.ci(bootstrap_mean_time_results, type = "bca")

ci_methods <- c("标准正态法", "基本法", "百分位法", "BCa 法")
lower_bounds <- c(norm_ci$normal[2], basic_ci$basic[4], perc_ci$percent[4], bca_ci$bca[4])
upper_bounds <- c(norm_ci$normal[3], basic_ci$basic[5], perc_ci$percent[5], bca_ci$bca[5])
data.frame(ci_methods,lower_bounds,upper_bounds)
```


分析与总结：

+ 置信区间范围⽐较：BCa法的置信区间最为宽泛，尤其是上限远⾼于其他⽅法。标准正态法和基本法的置
信区间较⼩，表明它们对不对称分布的处理能⼒有限。百分位法和BCa法则较为灵活，能够捕捉数据的极端情况。

+ 适⽤性⽐较：在实际应⽤中，BCa法和百分位法更适合数据分布可能偏斜、不对称的情况。BCa法由于进⾏了偏斜和加速的调整，因此是四种⽅法中最为稳健的选择。标准正态法和基本法在样本量较⼩或数据不正态分布时可能效果不佳。

# Homework5 - 10-23

## Exercise 7.8 

### Question

+ 导入数据集scor,并且计算样本估计量$\hat{\theta} = \frac{\hat{\lambda_1}}{\sum_{j=1}^{5} \hat{\lambda_j}}$
+ 利用bootstrap估计，并且计算偏差和标准误

### Answer

导入数据

```{R}
library(bootstrap)
data = data(scor)
```

计算原始数据的样本估计结果

```{R}
# 主成分分析
original_pca <- prcomp(scor, center = TRUE, scale. = TRUE)
# 提取前五个主成分的特征值
explained_variances <- (original_pca$sdev)^2
# 计算原始数据的PC1在前五个主成分的比例
original_ratio <- explained_variances[1] / sum(explained_variances[1:5])
```


进行数据重采样与bootstrap方法计算偏差和标准误

```{R}
# 假设scor是一个数据框，含有数据集的样本
set.seed(123)  # 设置随机种子
n <- nrow(scor)  # 获取数据集的行数
# 重复次数
N = 1000
boot_ratios <- numeric(N)

for (i in 1:N){
  # 有放回的抽样
  bootstrap_sample <- scor[sample(1:n, n, replace = TRUE),]
  # 执行主成分分析
  boot_pca <- prcomp(bootstrap_sample, center = TRUE, scale. = TRUE)
  # 提取前五个主成分的特征值
  explained_variances <- (boot_pca$sdev)^2
  # 计算原始数据的PC1在前五个主成分的比例
  boot_ratios[i] <- explained_variances[1] / sum(explained_variances[1:5])
}
# 计算偏差
bias <- mean(boot_ratios) - original_ratio
# 计算标准误
standard_error <- sd(boot_ratios)

cat("原始数据的样本估计值为", original_ratio,'\n')
cat("bootstrap方法的偏差为", bias,'\n')
cat("bootstrap方法的标准误为",standard_error)
```


## Exercise 7.10

### Question

+ 在例子7.18中，利用cubic polynomial model替换Log-Log model
+ 比较四个模型中的哪一个被交叉验证程序选择，并且根据最大调整$R^2$选择哪个模型？

### Answer

获取数据

```{R}
library(DAAG)
attach(ironslag)
```

下面对以下模型进行模型评估：
$$
Y = \beta_0+\beta_1X+\epsilon
$$
$$
Y = \beta_0+\beta_1X+\beta_2X^2+\epsilon
$$
$$
Y = log(\beta_0)+\beta_1X+\epsilon
$$
$$
Y = \beta_0+\beta_1X+\beta_2X^2+\beta_3X^3+\epsilon
$$
```{R}
a <- seq(10, 40, .1)
par(mfrow = c(2, 2))

L1 <- lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear", pch=16)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)
L2 <- lm(magnetic ~ chemical + I(chemical^2))
plot(chemical, magnetic, main="Quadratic", pch=16)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd=2)
L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd=2)
L4 <- lm(magnetic ~ chemical + I(chemical^2)+ I(chemical^3))
plot(chemical, magnetic, main="cubic", pch=16)
yhat4 <- L4$coef[1] + L4$coef[2] * a + L4$coef[3] * a^2 + L4$coef[4] * a^3
lines(a, yhat4, lwd=2)
```
应用交叉验证进行判断

```{R}
n <- length(magnetic)
e1 <- e2 <- e3 <- e4 <- numeric(n)
for (k in 1:n) {
  y <- magnetic[-k]
  x <- chemical[-k]
  
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
  e1[k] <- magnetic[k] - yhat1
  
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] + J2$coef[3] * chemical[k]^2
  e2[k] <- magnetic[k] - yhat2
  
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  e3[k] <- magnetic[k] - yhat3
  
  J4 <- lm(y ~ x + I(x^2) + I(x^3))
  yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k] + J4$coef[3] * chemical[k]^2 + J4$coef[4] * chemical[k]^3
  e4[k] <- magnetic[k] - yhat4
}
# 根据误差比较
cat("均值为", c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)),'\n')
cat("R^2为", c(summary(J1)$r.squared,summary(J2)$r.squared,summary(J3)$r.squared,summary(J4)$r.squared))
```
根据误差结果三次多项式是最佳选择。根据最大调整$R^2$的结果二次多项式是最佳选择。

## Exercise 8.1

### Question

+ 将相等分布的双样本Cramér-von Mises检验作为置换检验测试例8.1的数据。

### Answer

```{R}
data("chickwts")
# 选择两个饲料组作为样本
feed1 <- chickwts$weight[chickwts$feed == "horsebean"]
feed2 <- chickwts$weight[chickwts$feed == "linseed"]

# Cramér-von Mises统计量计算函数
cramer_von_mises <- function(x, y) {
  n1 <- length(x)
  n2 <- length(y)
  # 计算经验分布函数
  ecdf_x <- ecdf(x)
  ecdf_y <- ecdf(y)
  # 合并并排序样本
  combined <- sort(c(x, y))
  # 计算Cramér-von Mises统计量
  stat <- sum((ecdf_x(combined) - ecdf_y(combined))^2) * (n1 * n2 / (n1 + n2))
  return(stat)
}
# 置换检验函数
cramer_von_mises_permutation_test <- function(x, y, n_permutations = 1000) {
  # 计算原始统计量
  observed_stat <- cramer_von_mises(x, y)
  # 合并样本
  combined <- c(x, y)
  # 初始化置换统计量向量
  permuted_stats <- numeric(n_permutations)
  # 进行置换
  for (i in 1:n_permutations) {
    permuted_sample <- sample(combined)
    new_x <- permuted_sample[1:length(x)]
    new_y <- permuted_sample[(length(x) + 1):length(combined)]
    permuted_stats[i] <- cramer_von_mises(new_x, new_y)
  }
  # 计算p值
  p_value <- mean(permuted_stats >= observed_stat)
  return(list(observed_stat = observed_stat, p_value = p_value))
}
# 进行置换检验
set.seed(123)
result <- cramer_von_mises_permutation_test(feed1, feed2, n_permutations = 1000)

# 输出结果
cat("Cramer-von Mises检验统计量结果:", result$observed_stat, "\n")
cat("P-value:", result$p_value, "\n")
```

从结果来看，由于$p=0.009<0.01$，因此两种饲料的鸡体重分布显著不同

## Exercise 8.2

### Question

+ 将独立性的二元斯皮尔曼秩相关检验作为置换检验。
+ 将置换检验的显著性水平与cor.检验报告的相同样本的p值进行比较。

### Answer

由于题目没有指定数据集，本题使用服从正态分布的随机变量来进行测试。

```{R}
# 生成示例数据
set.seed(123)
x <- rnorm(50)
y <- rnorm(50)

# 定义置换检验函数
spearman_permutation_test <- function(x, y, n_permutations = 1000) {
  # 计算原始样本的Spearman相关系数
  observed_stat <- cor(x, y, method = "spearman")
  # 初始化置换统计量向量
  permuted_stats <- numeric(n_permutations)
  # 进行置换
  for (i in 1:n_permutations) {
    permuted_y <- sample(y)  # 随机打乱y
    permuted_stats[i] <- cor(x, permuted_y, method = "spearman")
  }
  # 计算置换p值
  p_value <- mean(abs(permuted_stats) >= abs(observed_stat))
  return(list(observed_stat = observed_stat, p_value = p_value))
}

# 运行置换检验
set.seed(123)
result_permutation <- spearman_permutation_test(x, y, n_permutations = 1000)

# 计算Spearman相关系数和p值
result_cor_test <- cor.test(x, y, method = "spearman")

# 输出结果
cat("Spearman相关系数:", result_permutation$observed_stat, "\n")
cat("Permutation test p值:", result_permutation$p_value, "\n")
cat("cor.test p值:", result_cor_test$p.value, "\n")
cat("p值的差距为：", abs(result_permutation$p_value-result_cor_test$p.value))
```
从结果上来看，p值的差距为0.02较为相近，两种方法在显著性水平上的一致性较高。但是在样本较小的时候，p值的差距较大。

# Homework6 - 10-28

## Exercise 9.3

### Question

+ 使用Metropolis-Hastings采样器生成服从标准柯西分布的随机变量。
+ 丢弃链的前1000个，观察生成的观测值的十分位数，并与标准柯西分布的十分位数进行比较

### Answer

服从$(\theta,\eta)$的柯西分布的密度函数为：
$$
f(x) = \frac{1}{\theta\pi(1+[(x-\eta)/\theta]^2)},\quad -\infty<x<\infty,\ \theta>0
$$
则标准柯西分布$(\theta=1,\eta=0)$的密度函数为：
$$
f(x) = \frac{1}{\pi(1+x^2)},\quad -\infty<x<\infty
$$
设置标准柯西分布的密度函数

```{R}
# 设置目标分布的密度函数
target_density <- function(x) {
  return (1/(pi*(1+x^2)))
}
```

定义采样函数，在该采样函数的定义中，提议分布是方差为1的正态分布

```{R}
# iter迭代次数，start是起始点
metropolis_hastings <- function(iter, start, proposal_sd) {
  k = 0
  samples <- numeric(iter) # 保存样本
  samples[1] <- start      # 初始值 
  for (i in 2:iter) {
    # 从提议分布中采样，生成新状态
    proposal <- rnorm(1, mean = samples[i - 1], sd = proposal_sd)
    # 计算接受率
    acceptance_ratio <- target_density(proposal) / target_density(samples[i - 1])
    # 以接受率的最小值为界进行判断
    if (runif(1) < acceptance_ratio) {
      samples[i] <- proposal # 接受提议样本
    } else {
      samples[i] <- samples[i - 1] # 拒绝提议，保留当前样本
      k <- k+1
    }
  }
  return(samples)
}
```

进行采样并可视化效果
```{R}
iterations <- 10000      # 迭代次数
start_value <- 0         # 起始点
proposal_sd <- 1         # 提议分布的标准差
set.seed(123)
samples <- metropolis_hastings(iterations, start_value, proposal_sd)

index <- 1000:10000
y1 <- samples[index]
plot(index, y1, type="l", main="", ylab="x")
```

进行十分位数的比较
```{R}
sample_deciles <- quantile(samples[1000:10000], probs = seq(0, 1, by = 0.1))

# 设置位置参数和尺度参数
location <- 0  # 默认位置参数
scale <- 1     # 默认尺度参数

# 计算十分位数
decile_probs <- seq(0, 1, by = 0.1)  # 十分位数对应的概率
deciles <- qcauchy(decile_probs, location, scale)

# 输出结果
results <- data.frame(
  Sample_Quantiles = sample_deciles,
  Theoretical_Quantiles = deciles,
  abs_diff = abs(sample_deciles-deciles)
)
print(results)
```
比较观察可知，采样器生成的结果的十分位数与理论值相近。

## Exercise 9.8

### Question

+ 使用Gibbs采样器生成服从双变量密度的链

### Answer

双变量密度表示如下：
$$
f(x,y)\propto\binom{n}{x}y^{x+a-1}(1-y)^{n-x+b-1},\quad x=0,1,...,n,\ 0\leq y\leq1
$$

由题目得到。当$a,b,n$确定时，条件分布是$Binomial(n,y)$和$Beta(x+a,n-x+b)$

$$
f(x|y)=Binomial(n,y)=\frac{n!}{y!(n-y)!}
$$

$$
f(y|x)=Beta(x+a, n-x+b) = \int_0^1 t^{x+a-1} (1-t)^{n-x+b-1} \, dt
$$

可以证明$y$的分布是$y \sim Beta(a,b)$

因此可以进行Gibbs采样如下

```{R}
N = 10000
X = matrix(0,N,2)
a = 2
b = 3
n = 10
burn = 1000

# 链的初始值
x0 <- rbinom(1, n, 0.5)  # 随机初始化 x
y0 <- rbeta(1,a,b)  # 随机初始化 y
X[1, ] <- c(x0, y0)

# 进行Gibbs采样
for (i in 2:N) {
  y <- X[i-1, 2]
  X[i, 1] <- rbinom(1,n,y)
  x <- X[i, 1]
  X[i, 2] <- rbeta(1,x+a,n-x+b)
}

# 丢弃前1000个数据
b <- burn + 1
x <- X[b:N, ]

# 可视化Gibbs采样
plot(X, main="", cex=.5, xlab='x',
  ylab='y', ylim=range(x[,2]))
```

## Exercise

### Question

+ 使用Gelman-Rubin method评估上两题的收敛性

### Answer

修改起始点来判断收敛速度

```{R}
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)
  #row means
  B <- n * var(psi.means)
  #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w)
  #within est.
  v.hat <- W*(n-1)/n + (B/n)
  #upper variance est.
  r.hat <- v.hat / W
  #G-R statistic
  return(r.hat)
}

set.seed(123)
sigma <- 1 #parameter of proposal distribution
k <- 4
#number of chains to generate
n <- 15000
#length of chains
b <- 1000
#burn-in length
#choose overdispersed initial values
x0 <- c(-1, 0, 1, 2)
#generate the chains
X <- matrix(0, nrow=k, ncol=n)

for (i in 1:k)
  X[i, ] <- metropolis_hastings(n, x0[i], sigma)

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
# 进行矩阵变化，psi是一个矩阵
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

#plot psi for the four chains
par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",
    xlab=i, ylab=bquote(psi))
par(mfrow=c(1,1)) #restore default

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.44, lty=2)
```


# Homework7 - 11.04

## Exercise 11.3

### Question

+ 编写一个函数计算函数的第$k$项
+ 计算$k$项的总和
+ 计算当$a=(1,2)^T$时的总和

### Answer

函数为：

$$
\sum_{k=0}^{\infty} \frac{(-1)^k}{k!2^k} \frac{\| a \|^{2k+2}}{(2k+1)(2k+2)} \frac{\Gamma(\frac{d+1}{2})\Gamma(k+\frac{3}{2})}{\Gamma(k+\frac{d}{2}+1)}
$$

该函数的第三项在$k$较大时会超出计算机的存储范围。需要将$Gamma$函数转换为$lgGamme$函数进行计算。

下面计算函数的第$k$项

```{R}
k = 100
a = c(1,2,3)
d = 10

# 计算第k项的函数
numk = function(k, a){
  l1 = (-1)^k/(factorial(k)*2^k)
  l2 = exp(log(norm(a, type = "2")^(2*k+2))-log(2*k+1)-log(2*k+2))
  l3 = exp(lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1))
  result = l1*l2*l3
  return(result)
}
```

对函数的$k$项求和

```{R}
num = 100
for (num in 1:20){
  k = 0:num
  odd = 2*(0:num)+1
  even = 2*(0:num)+2
  i = rep(c(1,-1),length = num+1)
  a = c(1,2)
  d = 10
  l3 = exp(lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1))
  l1 = i/factorial(k)/2^k
  l2 = norm(a, type = "2")^even/odd/even
  print(sum(l1*l2*l3))
}
```

## Exercise 11.5

### Question

+ 写一个函数解方程
+ 与练习11.4中的值进行对比

### Answer

需要解的方程为：

$$
\frac{2\Gamma(\frac{k}{2})}{\sqrt{\pi(k-1)}\Gamma(\frac{k-1}{2})}\int_{0}^{c_{k-1}}(1+\frac{u^2}{k-1})^{-k/2}du
=\frac{2\Gamma(\frac{k+1}{2})}{\sqrt{\pi k}\Gamma(\frac{k}{2})}\int_{0}^{c_{k}}(1+\frac{u^2}{k})^{-(k+1)/2}du
$$

对于$a$，有

$$
c_k = \sqrt{\frac{a^2k}{k+1-a^2}}
$$

求解方程我们可以将其转化为方程零点的求解，然后运用内置函数`uniroot`，求解变量是a

$$
\frac{2\Gamma(\frac{k}{2})}{\sqrt{\pi(k-1)}\Gamma(\frac{k-1}{2})}\int_{0}^{c_{k-1}}(1+\frac{u^2}{k-1})^{-k/2}du
-\frac{2\Gamma(\frac{k+1}{2})}{\sqrt{\pi k}\Gamma(\frac{k}{2})}\int_{0}^{c_{k}}(1+\frac{u^2}{k})^{-(k+1)/2}du = 0
$$

```{R}
c_k = function(a,k){
  result = sqrt(a^2*k/(k+1-a^2))
  return(result)
}

integral = function(u,k){
  (1+u^2/(k-1))^(-k/2)
}

expr = function(a,k){
  integral1 = function(u){
    integral(u,k)
  }
  c = c_k(a,k-1)
  l1 = 2*exp(lgamma(k/2)-lgamma((k-1)/2))/sqrt(pi*(k-1))
  res = l1 * integrate(integral1,lower = 0, upper = c)$value
  return(res)
}

# 进行迭代
result1 = function(a){
  expr(a,k) - expr(a,k+1)
}

resl = function(k){
  r = uniroot(result1, c(0.01,sqrt(k)-0.01))$root
  return(r)
}

for (k in c(4:15)){
  print(resl(k))
}
```

## Exercise

### Question

+ 使用E-M算法来估计右截断的指数分布参数λ
+ 将结果与MLE算法进行比较

### Answer

```{R}
# 初始化观测数据
Y <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau <- 1  # 截断值

# E-M算法的实现
EM_algorithm <- function(Y, tau, max_iter = 100, tol = 1e-6) {
  n <- length(Y)
  
  # 初始化lambda
  lambda <- 1 
  
  # 进行E-M迭代
  for (iter in 1:max_iter) {
    # E步：计算潜在变量T的期望
    T_expect <- rep(NA, n)
    for (i in 1:n) {
      if (Y[i] < tau) {
        T_expect[i] <- Y[i]
      } else {
        T_expect[i] <- tau + 1 / lambda
      }
    }
    
    # M步：最大化对数似然函数，更新lambda
    # 计算新的lambda
    lambda_new <- n / sum(T_expect)
    
    # 检查是否收敛
    if (abs(lambda_new - lambda) < tol) {
      break
    }
    
    # 更新lambda
    lambda <- lambda_new
  }
  
  return(lambda)
}

# 运行E-M算法来估计lambda
lambda_EM <- EM_algorithm(Y, tau)
cat("E-M估计的lambda值:", lambda_EM, "\n")

# MLE的估计
MLE_lambda <- 2*sum(Y[Y < tau]) / length(Y[Y < tau])
cat("MLE估计的lambda值:", MLE_lambda, "\n")
```


# Homework8 - 11-11

## Exercise 11.7

### Question

+ 使用单纯形法求解在一定的限制条件下算式$4x+2y+9z$的最小值

### Answer

问题为：

$$
min(4x+2y+9z)\\
2x+y+z \leq 2\\
x-y+3z \leq 3\\
x \geq 0,y \geq 0,z \geq 0
$$

下面使用单纯形法求解

```{R}
library(boot)
A1 = rbind(c(2, 1, 1), c(1, -1, 3))
b1 <- c(2, 3)
a <- c(4, 2, 9)
simplex(a = a, A1 = A1, b1 = b1, maxi = FALSE)
```

## Exercise p204_3

### Question

+ 使用循环和$lapply()$将此列表中存储的公式拟合线性模型，数据集使用$mtcars$。

### Answer

首先加载$mtcars$数据集，然后用循环和函数$lapply()$进行数据分析。

首先使用函数$lapply()$

```{R}
data(mtcars)
formulas <- list(
  mpg ~ disp, # 探索线性关系
  mpg ~ I(1 / disp), # 探索倒数关系
  mpg ~ disp + wt, # mpg与disp和wt的线性关系
  mpg ~ I(1 / disp) + wt # mpg 与 disp 的倒数和 wt 的线性关系
)
# 使用 lapply() 循环应用每个公式
models_ex3 <- lapply(formulas, function(formula) {
  lm(formula, data = mtcars)  # 拟合线性回归模型
})
# 打印模型摘要
models_ex3
```

然后使用循环进行分析

```{R}
# 创建一个空列表来存储模型
models_loop_ex3 <- list()

# 使用for循环拟合线性模型
for (i in seq_along(formulas)) {
  models_loop_ex3[[i]] <- lm(formulas[[i]], data = mtcars)
}

print(models_loop_ex3)
```

## Exercise p204_4

### Question

+ 使用循环和$lapply()$并使用线性模型拟合mpg和disp，对数据集mtcars进行自举重复，且不使用匿名函数。

### Answer

```{R}
set.seed(123)
# 定义 bootstrap 列表
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), replace = TRUE)
  mtcars[rows, ]
})

# 使用线性模型拟合
fit_model <- function(data) {
  lm(mpg ~ disp, data = data)
}

# 使用 lapply 将 fit_model 函数应用于每个 bootstrap 复制
models_ex4 <- lapply(bootstraps, fit_model)

# 创建一个空列表来存储模型
models_loop_ex4 <- list()

j = 0
for (i in bootstraps){
  j = j+1
  models_loop_ex4[[j]] = lm(i$mpg ~ i$disp,data = i)
}

print(models_ex4)
print(models_loop_ex4)

```

## Exercise p204_5

### Question

+ 对于exercise3和exercise4，提取结果中的$R^2$

### Answer

对于exercise3和exercise4中的模型结果进行计算

```{R}
rsq <- function(mod) summary(mod)$r.squared
# 对于ex3的模型
rex3 = lapply(models_ex3,rsq)
rex3_loop = lapply(models_loop_ex3,rsq)
rex4 = lapply(models_ex4,rsq)
rex4_loop = lapply(models_loop_ex4,rsq)
df1 = data.frame(
  Name = c("mpg ~ disp","mpg ~ I(1 / disp)","mpg ~ disp + wt","mpg ~ I(1 / disp) + wt"),
  lapply_R2 = as.double(unlist(rex3)),
  loop_R2 = as.double(unlist(rex3_loop))
)
df2 = data.frame(
  Name = c("bootstraps1","bootstraps2","bootstraps3","bootstraps4","bootstraps5","bootstraps6",
           "bootstraps7","bootstraps8","bootstraps9","bootstraps10"),
  lapply_R2 = as.double(unlist(rex4)),
  loop_R2 = as.double(unlist(rex4_loop))
)
print(df1)
print(df2)
```

## Exercise p213_3

### Question

+ 利用题目中所给的方法模拟非正态数据的t检验性能。
+ 使用`sapply()`和匿名函数从每个试验中提取p值。

### Answer

首先对题目中的代码进行分析

```{R}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
```

`rpois(x, y)`表示生成x个服从泊松分布的随机数，均值为y。`t.test()`表示对这两个样本执行独立样本t检验。
在`simplify = FALSE`下，`sapply()`类似于`lapply()`。

下面使用匿名函数和`sapply()`提取p值如下：

```{R}
sapply(trials, function(ttest) ttest$p.value)
```

## Exercise p214_6

### Question

+ 实现`Map()`和`vapply()`的组合，以创建一个`lapply()`变量，该变量对所有输入进行迭代，并将其输出存储在一个向量（矩阵）中。

### Answer

`Map()`是一个内置函数，用于对一个可迭代对象中的每个元素应用给定的函数，然后返回一个迭代器。
`vapply()`函数可以确定输出的格式。

使用`Map()`和`vapply()`的组合实现数组中元素的平方求解。

```{R}
my_list <- list(1, 2, 3, 4, 5)

# 使用 Map() 和 vapply() 实现 lapply() 的功能
result <- Map(function(x) vapply(x, function(y) y^2, numeric(1)), my_list)

data.frame(
  before = as.double(unlist(my_list)),
  after = as.double(unlist(result))
)
```

## Exercise p365_4_5

### Question

+ 实现一个卡方检验统计量的计算函数，比R语言中的内置函数更好。
+ 优化`table()`函数，是否能够进一步改进卡方检验统计量的计算。

### Answer

实现两个没有缺失值的数字向量的卡方检验统计量的计算

计算思路：

+ 生成两个没有缺失值的数字向量$x,y$,并分析它们之间的显著关系。
+ 计算数据框的行列和。
+ 计算对应的期望值。
+ 计算实际观测值与期望值之间的偏差。

```{R}
x = c(24,12,18)
y = c(31,71,45)
data = data.frame(x,y)
# 计算行列和
datasum = sum(data)
# 按列求和
sr <- rowSums(data)
# 按行求和
sc <- colSums(data)
# 计算对应的期望值
E = outer(sc,sr)/datasum
# 计算实际观测值与期望值的偏差和
chisq = sum((t(as.matrix(data))-E)^2/E)
chisq
```

改进`table()`函数，使用匿名函数和`sapply()`实现。

改进思路：

+ 转换为集合，确定出现过的元素
+ 利用函数`sapply()`确定元素的数量

```{R}
Fruit = c("Apple", "Banana", "Apple", "Orange", "Apple", "Banana", "Orange","Apple", "Banana", "Apple", "Orange", "Apple",
          "Apple", "Orange", "Apple")
Gender = c("Male", "Female", "Female", "Male", "Female", "Male", "Female","Female", "Male", "Female","Female", "Male",
           "Female","Male","Male")

data = data.frame(Gender, Fruit)
# 确定出现的元素
fruituni = unique(Fruit)
genderuni = unique(Gender)

# 计算出现的数量
count_occurrences <- function(x, B) {
  sum(x == B)
}

Fru_counts_male <- sapply(fruituni, count_occurrences, B = subset(data, Gender == "Male"))
Fru_counts_female <- sapply(fruituni, count_occurrences, B = subset(data, Gender == "Female"))

data.frame(Fru_counts_female,Fru_counts_male)
```


# Homework9 - 11-24

## Exercise 1

### Question

+ 对书本中Exercise9.8中题目使用Rcpp实现

### Answer

使用Gibbs采样双变量分布

```{R}
library(Rcpp)
library(SA24204125)

# 设置参数
N <- 10000
a <- 2
b <- 3
n <- 10
burn <- 1000

# 运行 Gibbs 采样
result <- gibbsCpp(N, a, b, n, burn)

# 查看结果
plot(result, main="", cex=.5, xlab='x',ylab='y', ylim=range(result[,2]))
```

## Exercise 2,3,4

### Question

+ 利用`qqplot`对比`R`生成的随机数和`cpp`生成的随机数
+ 利用`microbenchark`比较两种程序的运行时间
+ 比较运行的结果

### Answer

在R中运行如下：

```{R}
N = 10000
a = 2
b = 3
n = 10
burn = 1000
set.seed(12345)

gibbs_R = function(N,a,b,n,burn){
  X = matrix(0,N,2)
  # 链的初始值
  x0 <- rbinom(1, n, 0.5)  # 随机初始化 x
  y0 <- rbeta(1,a,b)  # 随机初始化 y
  X[1, ] <- c(x0, y0)
  
  # 进行Gibbs采样
  for (i in 2:N) {
    y <- X[i-1, 2]
    X[i, 1] <- rbinom(1,n,y)
    x <- X[i, 1]
    X[i, 2] <- rbeta(1,x+a,n-x+b)
  }
  
  # 丢弃前1000个数据
  b <- burn + 1
  x <- X[b:N, ]
}

X = gibbs_R(N,a,b,n,burn)
# 可视化Gibbs采样
plot(X, main="", cex=.5, xlab='x',ylab='y', ylim=range(X[,2]))
```


比较两种方法生成的QQ图

```{R}
par(mfrow = c(1, 2))
qqplot(X[,1], result[,1], main = "QQ Plot of X", xlab = "X from R", ylab = "X from Cpp")
abline(0, 1, col = "red", lwd = 2)  # 添加 y = x 的参考线
qqplot(X[,2], result[,2], main = "QQ Plot of Y", xlab = "Y from R", ylab = "Y from Cpp")
abline(0, 1, col = "red", lwd = 2)  # 添加 y = x 的参考线
```


可以发现用两种方法生成的数据的分布是一样的。

下面，比较两种方法的运行时间：

```{R}
library(microbenchmark)
library(SA24204125)
ts<-microbenchmark(Rgibbs = gibbs_R(N,a,b,n,burn), Cppgibbs = gibbsCpp(N, a, b, n, burn))
summary(ts)[,c(1,3,5,6)]
```

对比结果可以发现，`Rcpp`的运行速度显著高于在`R`本身中运行。

通过以上对比以及可视化结果可以发现，从结果的准确度来看，两种方式都可以生成准确的数据，但是使用Rcpp运行可以显著提高运行速度。







